{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\qquad$ $\\qquad$ $\\qquad$                                 **DAT405 Introduction to Data Science and AI, 2019-2020, Reading Period 3** <br />\n",
    "$\\qquad$ $\\qquad$ $\\qquad$                                 **Assignment 5: Reinforcement learning and Classification 2** <br />\n",
    "$\\qquad$ $\\qquad$ $\\qquad$                                 **Grader: Newton** <br />\n",
    "$\\qquad$ $\\qquad$ $\\qquad$                                 **Due Date: 23rd February, 23:59** <br />\n",
    "$\\qquad$ $\\qquad$ $\\qquad$                                 **Submitted by: Erling Hjermstad, 19990118-T454, erlingh@student.chalmers.se** <br />\n",
    "$\\qquad$ $\\qquad$ $\\qquad$ $\\qquad$ $\\qquad$ $\\quad$       **Fredrik Lilliecreutz, 19970407-1795, fredriklilliecreutz@gmail.com** <br />\n",
    "$\\qquad$ $\\qquad$ $\\qquad$                                 **Hours spent: 10h each** <br />\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "General guidelines:\n",
    "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "*   Your name, personal number and email address should be specified above.\n",
    "*   All tables and other additional information should be included in this notebook.\n",
    "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-check \n",
    "1. Have you answered all questions to the best of your ability? \n",
    "2. Anything else you can easily check? (details, terminology, arguments, commenting for code etc.?) \n",
    "\n",
    "Grading will be based on a qualitative assessment of each assignment. It is important to:\n",
    "*\tPresent clear arguments\n",
    "*\tPresent the results in a pedagogical way\n",
    "*\tShow understanding of the topics (e.g, write a pseudocode) \n",
    "*\tGive correct solutions\n",
    "*\tMake sure that the code is well commented \n",
    "\n",
    "**Again, as mentioned in general guidelines, all code should be written here. And this same ipython notebook file (RLAssignment.ipynb) should be submitted with answers and code written in it. NO SEPERATE FILE SHALL BE ACCEPTED.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primer\n",
    "\n",
    "## Decision Making\n",
    "The problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\n",
    "two parts. First, how do we learn about the world? This involves both the\n",
    "problem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\n",
    "currently know about the world, how should we decide what to do, taking into\n",
    "account future events and observations that may change our conclusions?\n",
    "Typically, this will involve creating long-term plans covering possible future\n",
    "eventualities. That is, when planning under uncertainty, we also need to take\n",
    "into account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\n",
    "things should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\n",
    "known to produce good results and experiment with something new is known\n",
    "as the **exploration-exploitation dilemma**.\n",
    "\n",
    "## The exploration-exploitation trade-off\n",
    "\n",
    "Consider the problem of selecting a restaurant to go to during a vacation.Lets say the\n",
    "best restaurant you have found so far was **Les Epinards**. The food there is\n",
    "usually to your taste and satisfactory. However, a well-known recommendations\n",
    "website suggests that **King’s Arm** is really good! It is tempting to try it out. But\n",
    "there is a risk involved. It may turn out to be much worse than **Les Epinards**,\n",
    "in which case you will regret going there. On the other hand, it could also be\n",
    "much better. What should you do?\n",
    "It all depends on how much information you have about either restaurant,\n",
    "and how many more days you’ll stay in town. If this is your last day, then it’s\n",
    "probably a better idea to go to **Les Epinards**, unless you are expecting **King’s\n",
    "Arm** to be significantly better. However, if you are going to stay there longer,\n",
    "trying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\n",
    "better food for the remaining time, while otherwise you will have missed only\n",
    "one good meal out of many, making the potential risk quite small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "* To make things concrete, we will first focus on decision making under **no** uncertainity, i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We shall first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n",
    "\n",
    "\n",
    "* Next, we will work through one type of reinforcement learning algorithm called Q-learning. Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process (MDP) provides a mathematical framework for modeling decision-making. It is a discrete time (distinct points in time) stochastic (randomly determined) process.\n",
    "\n",
    "MDPs are made up of 4 parts:  \n",
    "S: Finite set of states (Ex: s<sub>1</sub>, s<sub>2</sub> ... s<sub>N</sub>)  \n",
    "A: Finite set of actions (Ex: North, South, East, West)  \n",
    "P<sub>a</sub>(s,s'): Probability that action *a* in state *s* at time *t* will lead to state *s'* at time *t + 1*  \n",
    "R<sub>a</sub>(s,s'): Immediate reward received after moving from state *s* to state *s'* by action *a*\n",
    "\n",
    "An agent acts in an MDP at time *t*, by taking certain action *a* in state *s*, going to state *s'*, and getting a reward *r* from the world. It then repeats the process for certain no. of times, either finite or infinite.\n",
    "\n",
    "We also include a $5^{th}$ part in the description of an MDP called Gamma $\\gamma$.  \n",
    "$\\gamma$: The discount factor between 0 (inclusive) and 1 (exclusive). This determines how much credit you want to give to the future. If you think that the future reward is as important as the current reward you would set this to 0.99999. If you don't care about the future rewards you would set this to 0 and you only care about the current reward. For example, if your discount factor is 0.8 and after 5 steps you get a reward of 4 the present value of that reward is $0.8^4 * 5$ or ~2.\n",
    "\n",
    "An MDP is a collection of states such that each state has a selection of actions associated with them. With each state-action pair comes a reward *r* (can be 0). Define a policy function: $\\pi: s \\rightarrow a$, which tells which action to take at each state.\n",
    "  \n",
    "We now use the famous dynamic programming equation, also known as Bellman Equation, to define optimality in an MDP. The following equation defines what we call the **value function** of state *s* following some fixed policy $\\pi$:  \n",
    "\n",
    "$$V^\\pi(s) = \\sum_{s'} P_{\\pi(s)}(s,s') [R_{\\pi(s)}(s,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "We call $V^\\pi$ as the value of policy $\\pi$.  \n",
    "  \n",
    "Now, to find the **optimal** policy you will need to find the action that gives the highest reward.  \n",
    "\n",
    "$$V^*(s) = max_a \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "A real world example would be an inventory control system. Your states would be the amount of items you have in stock. Your actions would be the amount to order. The discrete time would be the days of the month. The reward would be the profit.  \n",
    "\n",
    "A major drawback of MDPs is called the \"Curse of Dimensionality\". This states that the more states/actions you have the more computational difficult it is to solve.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first question of the notebook, we give a quick example of an MDP. We would to see if you can put the definitions above into practice.\n",
    "\n",
    "**Question a**: Given the following deterministic MDP (you select North, you move North), what is the optimal policy (path with the most points)?\n",
    "  \n",
    "*Notes*:  \n",
    "  * The number in the box is the reward.  \n",
    "  * Once you hit the end you are done. (Absorbing state)\n",
    "  * S is the starting point.  \n",
    "  * F is the ending point.  \n",
    "  * Use N for North, E for East, S for South, and W for West. Not all actions are available at each state, for example, you can't choose N and W at starting state, as there exists no valid next states in those directions.  \n",
    "  * Pass the directions as a single string. Ex: ESWN will make a circle.  \n",
    "  \n",
    "\n",
    "\n",
    "| | | |\n",
    "|----------|----------|---------|\n",
    "|S|1|1|\n",
    "|1 |0|1|  \n",
    "|-1|-1|0|  \n",
    "|0 |0|F|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer a**: Interestingy the answer is independent of the factor $\\gamma$. The optimal policy is to go back and forth between two states which give a reward of 1 as this yields an infinite score. However, this sequence will never terminate and the game never finish.\n",
    ">\n",
    ">With $\\gamma > 0$ we will start east and get trapped in the following 1s.\n",
    ">E.g. E followed by EWEWEWEWEW..... or a similar pattern of 1s.  \n",
    ">With $\\gamma = 0$ we might start south as we disregard the future rewards, and thus don't care that our next choice will give zero reward instead of 1. However we might also head east as this yields the same immediate reward. What action is taken depends on the implementation.\n",
    ">\n",
    ">Policy for $\\gamma = 0$:\n",
    "> | | | |\n",
    "> |--|--|--|\n",
    "> |E/S|E|S/W|\n",
    "> |N/E|N/E/W|N|\n",
    "> |N|N/E/S|N|\n",
    "> |E|E/W|N/W|\n",
    ">\n",
    ">\n",
    ">Policy for $\\gamma > 0$:\n",
    "> | | | |\n",
    "> |--|--|--|\n",
    "> |E|E|S/W|\n",
    "> |N/E|N/E|N|\n",
    "> |N|N/E|N|\n",
    "> |E|E|N|\n",
    ">\n",
    "> Where '/' symbolizes equality between actions.\n",
    ">\n",
    ">If we are only allowed to visit each box once the answer changes. Then the best policy is SENESS as it visits all boxes with reward 1 and no negative boxes, unless we have a discount factor $\\gamma < 0.75$ where EESSS is the best policy since we get more rewards faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question b,c will attempt to firm up your knowledge of the parts of an MDP. Just remember that for a state denoted by (x,y), state N/E/S/W to that are (x,y-1),(x+1,y),(x,y+1),(x-1,y) respectively. We take (0,0) as the starting state S.\n",
    "\n",
    "**Question b:** What is the probability of going from state (1,0) to state (2,0) using action E ? ( i.e,  $P_E((1,0),(2,0))$ )\n",
    "\n",
    "**Question c:** What is the reward for moving from state (1,0) to state (2,0) ? ( i.e, $R_E((1,0),(2,0))$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer b**: The game is deterministic, so the probability of ending in state (2, 0) by choosing action E in (1, 0) is 1. There is no chance involved.\n",
    ">\n",
    ">**Answer c**: The reward is given by the number in box the box we arrive in. Here it is box (2, 0), so the reward is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value iteration is one algorithm that can be used to find the optimal policy ($\\pi^*$). Note that for any policy $\\pi^*$ to be optimal, it must satisfy the Bellman equation for optimal value function $V^*$. For any candidate $V^*$, it must be such that plugging it in the RHS (right-hand-side) of Bellman equation should give the same $V^*$ again (by the recursive nature of this equation). This property will form the basis of our algorithm. Essentially, due to certain mathematical results, repeated application of RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of Bellman equation for optimal value function will also lead to optimal value function, we can then extract the optimal actions by simply noting the actions that satisfy the equation.    \n",
    "\n",
    "The value function is based on the Bellman Equation for optimal value, which we recall here:  \n",
    "$$V^*(s) = max_a \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Below is a 3x3 grid. We are going to walk through a few iterations to firm up your understanding. Lets assume this time that success of taking any action is 0.8. Meaning if we take E from a valid state (x,y), we will go (x+1,y) 80 percent of time, but remain in same state the remaining time. We will have a discount factor ($\\gamma$) of 0.9. Assume $V^0(s')=0$ for all s'. \n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|0|0|\n",
    "|0|10|0|  \n",
    "|0|0|0|  \n",
    "\n",
    "\n",
    "**Iteration 1**: It is trivial, V(s) becomes the $max_a \\sum_{s'} P_a(s,s') R_a(s,s')$ since $V^0$ was zero for s'.\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|0|8|0|\n",
    "|8|2|8|  \n",
    "|0|8|0|  \n",
    "  \n",
    "**Iteration 2**:  \n",
    "  \n",
    "Staring with cell (0,0): We find the expected value of each move:  \n",
    "Action N: 0  \n",
    "Action E: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action S: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \n",
    "Action W: 0\n",
    "\n",
    "Hence any action between E and S would have been best at this stage.\n",
    "\n",
    "Similarly for cell (1,0):\n",
    "\n",
    "Action S: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action S is the maximizing action)  \n",
    "\n",
    "Similar calculations for remaining cells give us:\n",
    "\n",
    "| | | |  \n",
    "|----------|----------|---------|  \n",
    "|5.76|10.88|5.76|\n",
    "|10.88|8.12|10.88|  \n",
    "|5.76|10.88|5.76|  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (4 points)\n",
    "Please code the value iteration algorithm just described here, and show the optimal value function of the above 3x3 grid problem at convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function:\n",
      "[[45.58472762 51.9198559  45.58472762]\n",
      " [51.9198559  48.02375201 51.9198559 ]\n",
      " [45.58472762 51.9198559  45.58472762]]\n",
      "Optimal policy function:\n",
      "[['E/S' 'S' 'S/W']\n",
      " ['E' 'N/E/S/W' 'W']\n",
      " ['N/E' 'N' 'N/W']]\n",
      "Number of iterations: 71\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards = np.array([[0, 0, 0], [0, 10, 0], [0, 0, 0]])\n",
    "success_rate = 0.8\n",
    "discount_factor = 0.9 #gamma\n",
    "tol = 1e-4 #Threshold to stop iterating. Stops when changes are less than this measured in 2-norm^2\n",
    "\n",
    "\n",
    "value_function =  np.zeros_like(rewards, dtype=float)\n",
    "prev_values = np.ones_like(value_function) #just something more than tol different from value_function so the loop will run atleast once\n",
    "optimal_action = np.zeros_like(value_function, dtype=object)\n",
    "\n",
    "\n",
    "sizeX = len(value_function[0])\n",
    "sizeY = len(value_function)\n",
    "\n",
    "actions = {'N' : {'x' : 0, 'y' : -1}, 'E' : {'x' : 1, 'y' : 0}, 'S' : {'x' : 0, 'y' : 1}, 'W' : {'x' : -1, 'y' : 0}}\n",
    "\n",
    "counter = 0\n",
    "while np.sum(np.square(value_function - prev_values)) > tol:\n",
    "    counter += 1\n",
    "    prev_values = np.copy(value_function)\n",
    "    for y in range(sizeY):\n",
    "        for x in range(sizeX):\n",
    "            max_value = -1\n",
    "            best_action = ''            \n",
    "            for dir in actions:\n",
    "                if x + actions[dir]['x'] >= sizeX or x + actions[dir]['x'] < 0:\n",
    "                    continue #action out of bounds\n",
    "                if y + actions[dir]['y'] >= sizeY or y + actions[dir]['y'] < 0:\n",
    "                    continue #action out of bounds\n",
    "                \n",
    "                #continously updating value_function so need to get values from prev_values\n",
    "                value = success_rate*(rewards[y + actions[dir]['y'], x + actions[dir]['x']] + discount_factor*prev_values[y + actions[dir]['y'], x + actions[dir]['x']]) + (1 - success_rate)*(rewards[y, x] + discount_factor*prev_values[y, x])\n",
    "                # print(\"Pos:\",i,j,\"Action:\", dir, \"Value:\", value)\n",
    "\n",
    "                if  value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = dir\n",
    "                elif value == max_value:\n",
    "                    best_action += '/' + dir\n",
    "\n",
    "            value_function[y, x] = max_value\n",
    "            optimal_action[y, x] = best_action            \n",
    "\n",
    "print(\"Optimal value function:\")\n",
    "print(value_function)\n",
    "print(\"Optimal policy function:\")\n",
    "print(optimal_action)\n",
    "print(\"Number of iterations:\", counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The optimal value function which maps states (postions) to values is on the form\n",
    "> | | | |\n",
    "> |---|---|---|\n",
    "> |45.6|51.9|45.6|\n",
    "> |51.9|48.0|51.9|\n",
    "> |45.6|51.9|45.6|\n",
    ">\n",
    ">\n",
    "> Thus the optimal policy function which maps states (positions) to actions is on the form\n",
    "> | | | |\n",
    "> |---|---|---|\n",
    "> |E/S|S|S/W|\n",
    "> |E|N/E/S/W|W|\n",
    "> |N/E|N|N/W|\n",
    ">\n",
    "> Where '/' symbolizes equality between actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning (RL)\n",
    "Until now, we understood that knowing the MDP, specifically $P_a(s,s')$ and $R_a(s,s')$ allows us to efficiently find the optimal policy using value iteration algorithm, but RL or decision making under uncertainity arises from the question of making optimal decisions without knowing the true world model (MDP in this case).\n",
    "\n",
    "So far we have defined the value of a state $V^\\pi$, let us define the value of an action, $Q^\\pi$:\n",
    "\n",
    "$$Q^\\pi(s,a) = \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "i.e, the value of taking action *a* from state *s* and then following $\\pi$ onwards. Similarly, the optimal Q-value equation is:\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s'} P_a(s,s') [R_a(s,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Q-learning algorithm can be used by an agent unaware of its surroundings (unknown MDP). All it can do is take an action *a* at time *t* from state *s* and observe the reward *r* and next state *s'*, and repeat this process again. So how it can learn to act optimally under such uninformative conditions ? Answer is using Q-learning. Without going into its justification, we simply state the main-update rule of this algorithm below:\n",
    "\n",
    "![alt text](https://chalmersuniversity.box.com/shared/static/5anbos4s9luoayb32jk6w3wy3w4jk3g3.png)\n",
    "\n",
    "Where we simply maintain Q(s,a) value for each state-action pair in a table. It is proven to converge to the optimal policy of the underlying unknown MDP for certain values of learning rate $\\alpha$. For our case, we set a constant $\\alpha=0.1$.\n",
    "\n",
    "## OpenAI Gym\n",
    "\n",
    "We shall use already available simulators for different environments (world) using the popular OpenAI Gym library. It just implements [differnt types of simulators](https://gym.openai.com/) including ATARI games. Although here we will only focus on simple ones, such as [Chain enviroment](https://gym.openai.com/envs/NChain-v0/).\n",
    "\n",
    "![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\n",
    "\n",
    "## Question 3 (1 point)\n",
    "Basically, there are 5 states, and two actions 'a' and 'b'. Each transition (s,a,s') is noted with its corresponding reward. You are to first familiarize with the framework using its [documentation](http://gym.openai.com/docs/), and then implement the Q-learning algorithm for the Chain enviroment (called 'NChain-v0') using default parameters. Finally print the $Q^*$ table at convergence. Take $\\gamma=0.95$. You can refer to the Q-learning Jupyter notebook shown in class, uploaded on Canvas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[55.01127554, 53.60247797],\n",
       "       [58.1299195 , 54.86465834],\n",
       "       [66.25433246, 58.18234438],\n",
       "       [72.92469948, 60.83355567],\n",
       "       [85.7497012 , 55.7919033 ]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import gym_toytext #resolve legacy issues with NChain-v0\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gamma = 0.95\n",
    "learning_rate = 0.1\n",
    "\n",
    "epsilon = 0.1 #how often we explore\n",
    "\n",
    "num_episodes = 1500 #how many iterations of the game is to be played. 1 game = 1000 choices\n",
    "\n",
    "env = gym.make('NChain-v0')\n",
    "\n",
    "num_actions = env.action_space.n #two possible actions. 0 or 1 / a or b / forward or back to start\n",
    "num_states = env.observation_space.n #5 possible states\n",
    "\n",
    "Q = np.zeros([num_states, num_actions]) #Q table\n",
    "prevQ = np.ones_like(Q)\n",
    "\n",
    "line = []\n",
    "\n",
    "##Code below is from lecture\n",
    "for i in range(num_episodes):\n",
    "\tstate = env.reset()\n",
    "\tdone = False\n",
    "\tprevQ = np.copy(Q)\n",
    "\n",
    "\twhile done == False:\n",
    "        # First we select an action:\n",
    "\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
    "\t\t\taction = env.action_space.sample() # Explore action space\n",
    "\t\telse:\n",
    "\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n",
    "        # Then we perform the action and receive the feedback from the environment\n",
    "\t\tnew_state, reward, done, info = env.step(action)\n",
    "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
    "\t\tprediction_error = reward + gamma*np.max(Q[new_state,:]) - Q[state, action]\n",
    "\t\tQ[state, action] += learning_rate*prediction_error \n",
    "\t\tstate = new_state\n",
    "\n",
    "\tline.append(Q[3, 0])\n",
    "\n",
    "plt.plot(list(range(num_episodes)), line)\n",
    "plt.xlabel(\"# of iterations\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Q-Value of action 0 in state 3\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "env.close()\n",
    "Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Can see from the output that moving forward is always the recommended action, and increasingly so as we move along the chain.\n",
    ">When letting the algorithm run until convergence, it runs forever. The Q-table has quite big variance from iteration to iteration, but as can be seen from the plot for action 0 in state 3 it quickly stabilizes around the same value. This is also the case for the other state-action pairs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 4 (2 points)\n",
    "\n",
    "a. Verify that the optimal $Q^*$ value obtained using Q-learning is same as the optimal value function $V^*$ for the corresponding MDP's optimal action. You would have to first define the MDP corresponding to Chain enviroment.\n",
    "\n",
    "b. What is the importance of exploration in RL ? Explain with an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer a:**\n",
    ">Were not able to find the default values of NChain-v0 as it has been removed from Gym and its sourcecode it also been removed from the gym repo on github. Assuming $\\gamma = 0.95$, the probabilities are $P_{\\text{Forward}}(n, n+1) = 95 \\% $, $P_{\\text{back to start}}(n, 0) = 95 \\% $, $P_{\\text{Forward}}(n, 0) = 5 \\% $ and $P_{\\text{back to start}}(n, n+1) = 5 \\%$. Given rewards as per the drawing this gives a $V^*(s)$ as can be seen at the ouput of following script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal value function:\n",
      "[[51.74207028 48.73249232]\n",
      " [57.67982084 49.04500551]\n",
      " [64.62455834 49.41051801]\n",
      " [72.74705834 49.83801801]\n",
      " [82.24705834 50.33801801]]\n",
      "Number of iterations: 76\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards = np.array([[0, 2], [0, 2], [0, 2], [0, 2], [10, 2]])\n",
    "success_rate = 0.95\n",
    "discount_factor = 0.9\n",
    "tol = 1e-4 #Threshold to stop iterating. Stops when changes are less than this measured in 2-norm^2\n",
    "\n",
    "\n",
    "value_function =  np.zeros_like(rewards, dtype=float)\n",
    "prev_values = np.ones_like(value_function) #just something more than tol different from value_function so the loop will run atleast once\n",
    "\n",
    "sizeX = len(value_function[0])\n",
    "sizeY = len(value_function)\n",
    "\n",
    "\n",
    "counter = 0\n",
    "while np.sum(np.square(value_function - prev_values)) > tol:\n",
    "    counter += 1\n",
    "    prev_values = np.copy(value_function)\n",
    "    for state in range(sizeY):\n",
    "        moving_on_reward = rewards[state, 0] + discount_factor*max(prev_values[min(4, state + 1)])  \n",
    "        back_to_start_reward = rewards[state, 1] + discount_factor*max(prev_values[0])       \n",
    "        #action 0/forward\n",
    "        value_function[state, 0] = success_rate*moving_on_reward + (1 - success_rate)*back_to_start_reward    \n",
    "        #action 1/back to start\n",
    "        value_function[state, 1] = success_rate*back_to_start_reward + (1 - success_rate)*moving_on_reward\n",
    "        \n",
    "           \n",
    "\n",
    "print(\"Optimal value function:\")\n",
    "print(value_function)\n",
    "print(\"Number of iterations:\", counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Can see that the Q-table and the $V^*$-table are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Answer b:**\n",
    ">If never explore potentially huge rewards might go undiscovered. Take the NChain environment, if we only know about the reward of going back to start, we will always choose this as there are no known rewards of moving along the chain. However, as we know the optimal strategy is to explore and find the big reward at the end of the chain. This has parallels to life. Maybe it is worth it to explore and deviate from ones habits from time to time to gain higher rewards in life. For example exploring new job opportunities in a new city to gain a higher salary and standard of living. Or you might dislike the new city and realised that your life satifaction was better in your previous city. You will never know unless you start exploring new opportunities, however too much exploring might also reduce the standard of living / rewards in life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (1 point)\n",
    "\n",
    "Briefly discuss the k-armed bandit problem formulation and it's distinguishing feature as a special case of the reinforcement learning problem formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:**\n",
    ">The K-armed bandit problem is an exploration/explotation problem just like the NChain. Imagine K armed bandit slot machines in a casino, each with an unknown machine-specific probability distribution for determining the reward of playing on the machine. The problem is how to maximize the reward from playing N times on the machines. By exploring a player can get an idea of which machines have the most favorable distributions and exploit this information in the remaining plays. It is an MDP with just one state and K actions and unknown rewards. Regardless of what action we take, we still are in the same state and all actions are available. This is a special feature of the K-armed bandit problem. The main challenge is to find the balance between exploring and exploiting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "* Until now, we have described algorithms for when no. of states and actions are finite. In coming weeks, you will be taught how to extend these methods to continous state enviroments like ATARI games.\n",
    "\n",
    "# References\n",
    "Primer/text based on the following references:\n",
    "* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n",
    "* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
